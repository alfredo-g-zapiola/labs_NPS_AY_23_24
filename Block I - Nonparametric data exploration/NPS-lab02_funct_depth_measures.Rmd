---
title: "Lab 02 - Depth measures for functional data"
date: 2023/09/26
author: "Nonparametric statistics ay 2023/2024"
output:
  
  html_document: 
    df_print: paged
  html_notebook: 
    df_print: paged
  pdf_document: default
  word_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{css, echo=FALSE}
.extracode {
background-color: lightblue;
}
```

## Loading necessary libraries

```{r message=FALSE, warning=FALSE}
library(roahd)
```

## Functional data analysis with R: a primer

Functional Data Analysis is one of the core research areas of our
department, you have already seen during class that some of the
state-of-the-art techniques were actually developed here. Depth-based
techniques find a natural application in Functional Statistics. I assume
that many of you already know what we mean by functional data (applied
statistics course *docet*), nevertheless do not worry if you do not: I
have structured this lab as a very basic primer to understand how to do
some methodological and applied work in the framework of functional data
analysis (FDA).

According to the FDA model, data can be seen as measurements of a
quantity/quantities along a given, independent and continuous indexing
variable (such as time or space). Observations are then treated as
random functions and can be viewed as trajectories of stochastic
processes defined on a given infinite dimensional functional space.

A natural way to reason (and actually comprehend why they need specific
and peculiar techniques) is to think about them as **relatively dense
longitudinal data**: unlike tabular data, functional data are not
invariant to permutations of their dimensions (i.e., if I switch two
columns of a standard multivariate dataset no harm is done, if I do it
with a functional one, I will create a bloody mess).

Given its ubiquitous presence in applications FDA has been a very active
research field in the past decades, with a plethora of R packages
developed for helping practitioner efficiently and effectively employ
such techniques. In what follows we will focus our attention on the
`roahd` (\*RObust Analysis of High Dimensional Data) package (developed
by members and former members of our Department of Mathematics): a
package meant to collect and provide methods for the analysis of
univariate and multivariate functional datasets through the use of
robust methods. First off we will dedicate some time in understanding
how to represent these complex infinite dimensional objects (our
computers only deal with finite approximations) and how to simulate
functional data. We will then focus on computation of depths and outlier
detection. This lab WILL NOT provide a thorough treatment of FDA, but we
will limit to study some introductory concepts on how to deal with
univariate (and bivariate, only very briefly) functional datasets.

## Simulating functional data with roahd package

The way `roahd` package represents functional objects is by providing an
evenly spaced grid $I=\left[t_{0}, t_{1}, \ldots, t_{P-1}\right]$
$\left(t_{j}-t_{j-1}=h>0, \: \forall j=1, \ldots, P-1\right)$ over which
the functional observations $D_{i, j}=X_{i}\left(t_{j}\right)$ (*i.e.*
the $i$th function evaluated at the $j$th point of the grid)
$\forall i=1, \ldots, N$ and $\forall j=0, \ldots, P-1$ are measured.
This is very conveniently handled by the `fData` object class. In
particular, the following model is considered for the generation of
data: $$
X(t)=m(t)+\varepsilon(t), \text { for all } t \text { in } I
$$ where $m(t)$ is the mean function (deterministic) and
$\varepsilon(t)$ is a centered Gaussian process with covariance function
$C(\cdot,\cdot)$. That is to say: $$
\operatorname{Cov}(\varepsilon(\mathrm{s}), \varepsilon(t))=C(s, t), \text { with } s, t \text { in } I
$$ The employed structure for $C(s, t)$ is the Exponential covariance
function:

$$
C(s, t)=\alpha e^{-\beta|s-t|}
$$ Where $alpha$ controls the point-wise variance, and $\beta$ the
degree of covariance between points.

```{r}
P <- 101
grid <-  seq( 0, 1, length.out =  P)
  
alpha <-  0.2
beta <-  0.2

C_st <- exp_cov_function( grid, alpha, beta )
dim(C_st)  # note of course it is a covariance matrix
```

`C_st` contains a $P \times P$ matrix of values.

```{r}
image( C_st,
       main = 'Exponential covariance function',
       xlab = 'grid', ylab = 'grid')
```

After having defined the mean function $m(t)$

```{r}
m <- sin(pi*grid)+sin(2*pi*grid)
```

we are ready to generate functional data as follows

```{r}
n <- 100
set.seed(26111992)
data <- generate_gauss_fdata(N = n,centerline = m,Cov=C_st)
dim(data) # 100 functions evaluated at 101 points each
```

The output of the previous chunk is actually a $n \times P$ matrix,
where $P$ is equal to the length of the grid. That is, instead of having
a "proper" functional datum, I have its evaluation on a relatively fine
grid (this is actually the best we can do). We can plot it with:

```{r}
matplot(grid,t(data), type="l", col=adjustcolor(col=1,alpha.f = .4))
lines(grid,m, col="blue", lwd=5)  # add something else on top of existing graph
```

Or even better we exploit the features of the `roahd` package to
construct a functional object:

```{r}
f_data <- fData(grid, data)
plot(f_data) # what happens if I do plot(data)?
lines(grid,m, col="black", lwd=5)
```

The first command defines an object of class fdata

```{r}
class(f_data)
```

with dedicated methods for plots, the four basic algebraic operations,
subsetting and much more. In particular, using objects of class `fdata`
makes the computation of depth measures and related quantities much
easier (see next section). For a thorough account on the potential of
the `roahd` package, check
[this](https://journal.r-project.org/archive/2019/RJ-2019-032/index.html)
paper.

By changing the hyperparameters $\alpha$ and $\beta$ in the Exponential
covariance function, we can generate functional datum with different
degree of dependence and variability. In details, $\alpha$ controls the
overall level of variability in the signal, while the parameter $\beta$
affects the autocorrelation length of the signal's noise, with lower
values of $\beta$ leading to wider correlation lengths and vice-versa

```{r}
alpha <-  1
beta <-  0.2

C_st <- exp_cov_function( grid, alpha, beta )

data <- generate_gauss_fdata(N = n,centerline = m,Cov=C_st)

f_data <- fData(grid,data)
plot(f_data, main="High overall level of variability")

alpha <-  .1
beta <-  0.0001

C_st <- exp_cov_function( grid, alpha, beta )

data <- generate_gauss_fdata(N = n,centerline = m,Cov=C_st)

f_data <- fData(grid,data)
plot(f_data, main="High smothness") 

alpha <-  .1
beta <-  100

C_st <- exp_cov_function( grid, alpha, beta )

data <- generate_gauss_fdata(N = n,centerline = m,Cov=C_st)

f_data <- fData(grid,data)
plot(f_data, main="Virtually uncorrelated signals") 
```

## Computing depth measures in a FDA framework

Let us now consider the data you have seen in the Case Study: ECG
signals. The registered and smoothed signals are contained in the
`mfD_healthy` object.

```{r}
data("mfD_healthy") # included in roahd
univariate_fdata <- mfD_healthy$fDList[[1]] # I consider the first lead only
print(class(univariate_fdata))  #fData object
plot(univariate_fdata) # can do it directly with an fData object
```

With `roadhd` it is very easy to compute Band depths and Modified band
depths for a given `fdata` object:

```{r}
band_depth <- BD(Data = univariate_fdata)
modified_band_depth <- MBD(Data = univariate_fdata)
```

We can compute the median curve

```{r}
median_curve <- median_fData(fData = univariate_fdata, type = "MBD") # still an fData object
```

Or manually by firstly computing the Modified Band Depth and then
identifying the curve with max MBD

```{r}
median_curve_manual <- univariate_fdata[which.max(modified_band_depth),] # still an fData object
```

```{r}
all(median_curve_manual$values==median_curve$values) 
```

```{r}
plot(univariate_fdata)
grid_ecg <- seq(median_curve_manual$t0,median_curve_manual$tP,by=median_curve_manual$h)
lines(grid_ecg,median_curve_manual$values)
```

### Parenthesis: the failure of multivariate depth functions in high dimensinons

What if we used the Tukey Median for a functional datum? Let's try it:

```{r}
set.seed(2)
n = 32
# generate functional datset
grid = seq(0,1,length.out=1000)
alpha <-  1
beta <-  0.2
C_st <- exp_cov_function( grid, alpha, beta )
m <- sin(pi*grid)+sin(2*pi*grid)
data <- generate_gauss_fdata(N = n,centerline = m,Cov=C_st)
f_data = fData(grid=grid, values=data)  # cast to fData
plot(f_data)
median.mbd =  median_fData(fData = f_data, type = "MBD")

lines(grid, median.mbd$values, col="black")
library(DepthProc)
tukey.depth=depth(u=f_data$values,method='Tukey')
tukey.deepest.idx = which(tukey.depth==max(tukey.depth))
lines(grid, f_data$values[tukey.deepest.idx[1],], col="red")

```

It does not seem like a correct estimator for location... This is due to
the fact that as $d$ increases, heuristic approaches are utilised for
classical depth measures (such as Tukey depth), yielding poor
estimation. Remember, in the multivariate setting, depth measures deal
with the "**center-outward order**", whereas in FDA we try to obtain a
"**down-upward order**".

### Spearman's correlation index for functiona data

Epigraph Index (EI) and Hypograph Index (HI) or their corresponding
Modified versions (MEI and MHI) for providing down-upward/up-downward
order of data can also be computed: see functions `EI`, `HI`, `MEI`,
`MHI`. As an example, let us compute the Spearman's correlation index
between the first and second lead of the ECG signals. Recall that for a
bivariate functional dataset $[\mathbf{x}, \mathbf{y}]$, the estimate
for Spearman's correlation index
$\hat{\rho}_{s}(\mathbf{x}, \mathbf{y})$ is given

$$
\hat{\rho}_{s}(\mathbf{x}, \mathbf{y})=\hat{\rho}_{p}\left(I L\operatorname{grade}_{n}(\mathbf{x}), I L\operatorname{grade}_{n}(\mathbf{y})\right)
$$ where $\hat{\rho}_{p}$ is the sample Pearson correlation coefficient
and $I L\operatorname{grade}_{n}$ is the Inferior-Length w.r.t the
sample and $$
\begin{aligned}
&I L_{n}-\operatorname{grade}(\mathbf{x})=\left(I L_{n}-\operatorname{grade}\left(x_{1}\right), I L_{n}-\operatorname{grade}\left(x_{2}\right), \ldots, I L_{n}-\operatorname{grade}\left(x_{n}\right)\right) \\
&I L_{n}-\operatorname{grade}(\mathbf{y})=\left(I L_{n}-\operatorname{grade}\left(y_{1}\right), I L_{n}-\operatorname{grade}\left(y_{2}\right), \ldots, I L_{n}-\operatorname{grade}\left(y_{n}\right)\right) .
\end{aligned}
$$ where the inferior length grade $I L\operatorname{grade}_{n}(x_i)$
can be interpreted as the "proportion of time" that the sample
$x_1,\ldots,x_n$ is smaller than $x_i$, that is it defines the relative
position of a curve with respect to the sample (it pretty much counts
what percentage of the domain curve $i$ is higher than other samples)

The Spearman's correlation index is immediately obtained via the
`cor_spearman` function

```{r}
bivariate_data <- as.mfData(list(mfD_healthy$fDList[[1]], mfD_healthy$fDList[[2]]))
plot(bivariate_data)
cor_spearman(bivariate_data, ordering='MHI')
```

Or actually we can manually compute it:

```{r, class.source="extracode"}
MHI_first_lead <- MHI(bivariate_data$fDList[[1]]) # modified hypograph
MHI_second_lead <- MHI(bivariate_data$fDList[[2]])

cor(MHI_first_lead, MHI_second_lead)
```

Coding tip: never forget the power of functional programming!

```{r, class.source="extracode"}
do.call(args = lapply(1:2, function(ind)
  MHI(bivariate_data$fDList[[ind]])) # list of arguments
  , what = "cor" # name of the function
  )
```

## Outlier detection with roahd: functional boxplots and outliergrams

We conclude this lab by looking at some graphical tools for identyfing
outliers in a functional sample. Consider the following univariate
functional data

```{r}
n = 100
alpha <-  0.2
beta <-  0.002

C_st <- exp_cov_function( grid, alpha, beta )

data <- generate_gauss_fdata(N = n,centerline = m,Cov=C_st)

f_data <- fData(grid,data)
```

We consider two cases: first a dataset with some magnitude outliers,
obtained inflating the last 10 curves by a number generated from a
uniform distribution in $[2,3]$

```{r}
set.seed(33) # reproducibility
outlier_share <- .1
n_outliers <-   n*outlier_share
out_highlighter <- rep(c(1,2),c(n-n_outliers,n_outliers))
f_data_temp <- f_data[1:(n*(1-outlier_share)),] # Coding tip: subsetting is mabe possible by the S3 class fdata
mag_temp <- f_data[(n*(1-outlier_share)+1):n,] * runif(10,2,3)

f_data_mag <- append_fData(f_data_temp,mag_temp)
plot(f_data_mag, col=out_highlighter)
```

And then some shape outliers by shifting the generating mechanism by the
quantity `shift_q`:

```{r}
shift_q <- .5

mu_warp=mu=sin(pi*grid+shift_q)+sin(2*pi*grid+shift_q)

shape_temp=generate_gauss_fdata(N = n_outliers, mu_warp, Cov=C_st)
shape_temp=fData(grid,shape_temp)
f_data_shape=append_fData(f_data_temp,shape_temp) 
plot(f_data_shape, col=out_highlighter)
```

Now let us see whether we can employ the two plotting tools we have seen
in class, namely functional boxplots and outliegrams to detect magnitude
and shape outliers, respectively.

```{r}
invisible(fbplot(f_data_mag, main="Magnitude outliers"))

```

Recall:

-   The central region ("functional bag") contains the functions with
    the highest MBD value
-   The central region is inflated by factor $F$ to obtain the fences.

I used `invisible()` in the previous chunk to avoid the output of the
call to be printed on the console (and in the knitted html file). If I
do not use it I obtain the following:

```{r}
fbplot(f_data_shape, main="Shape outliers")

```

Now, we plot the outliergram.

```{r}
outliergram(f_data_shape)
```

We appreciate how the shape outliers are almost entirely missed by the
functional boxplot, whereas the outliergram effectively uncovers them.
Recall the important result:

$$
MBD(f) \leq a_0 + a_1 MEI(f) + a_2 N^2 MEI^2(f)
$$ where MBD represents the **Modified Band Depth** and MEI the
**Modified Epygraph Index**.

As usual, saving the output of the plots to an object allows to recover
the ID of the identified outliers:

```{r}
out_shape <- outliergram(f_data_shape, display = FALSE)
out_shape$ID_outliers
```

The outliergram is actually based on some simple manipulations of MEI
and MBD, that can be hard coded:

```{r, class.source="extracode"}
MEI_out_shape <- MEI(f_data_shape)
MBD_out_shape <- MBD(f_data_shape)
a_0 <- a_2 <- -2/(f_data_shape$N*(f_data_shape$N-1))
a_1 <- 2*(f_data_shape$N+1)/(f_data_shape$N-1)
d_manual <- a_0+a_1*MEI_out_shape+a_2*f_data_shape$N^2*MEI_out_shape^2-MBD_out_shape
ID_outliers_manual <- which(d_manual>quantile(d_manual,probs = .75)+out_shape$Fvalue*IQR(x = d_manual))
```

Let us check that our manual computations are correct:

```{r, class.source="extracode"}
all(dplyr::near(d_manual, out_shape$d))
all(ID_outliers_manual==out_shape$ID_outliers)
```

One last comment: we have seen in class that the value $F$ used to build
the fences in the functional boxplot can be adjusted. Despite the
algorithmic implementation/ mathematical theory behind it being quite
tedious, the automated selection of $F$ via the `roahd` package is
actually pretty easy:

```{r message=FALSE, warning=FALSE, evNal=FALSE}
set.seed(22)
fbplot(f_data_mag, main="Magnitude outliers",adjust = list(VERBOSE=FALSE))
outliergram(f_data_mag,adjust = list(VERBOSE=FALSE))
```
