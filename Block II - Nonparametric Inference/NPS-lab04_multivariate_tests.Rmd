---
title: "Lab 04 - Permutational Multivariate Tests"
author: "Nonparametric statistics ay 2023/2024"
date: "2023/10/11"
output:
  html_document: default
---

```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{css, echo=FALSE}
.extracode {
background-color: lightblue;
}
```



```{r}
B = 1e5
seed = 26111992
```


## Permutational Multivariate Tests

In this part of the lab, we will directly implement some permutational multivariate tests, applied to specific problems
Let's start with our first one...

## Two multivariate populations test

We are given data of the week 12/09/2016 -> 18/09/2016 about the number of vehicles that, each half-hour, enter Milan Low emission zone (the so called "_area C_"). You want to know if the mean entrance pattern in the weekends is significantly different from the one in the weekdays.
Framed in a more rigorous way, being 

$$
\mathbf{Y}_{i,w} \overset{iid}{\sim} \mathbf{Y}_{w} \in \mathbb{R}^{48}; \, i \in \{1,...,5\}
$$ 
where $w$ denotes the distribution of weekdays, and $i$ is the $i$th day of the week. We also have:
$$
\mathbf{Y}_{i,\tilde{w}} \overset{iid}{\sim} \mathbf{Y}_{\tilde{w}} \in \mathbb{R}^{48} \; i \in \{6,7\}
$$
where $\tilde{w}$ is to denote weekend day.
We want to test the equality of the two distributions, namely, we want to devise a test of the type:
$$
H_0: \mathbf{Y}_{w} \overset{d}{=} \mathbf{Y}_{\tilde{w}}\;vs\;H_1:\mathbf{Y}_{w} \overset{d}{\neq} \mathbf{Y}_{\tilde{w}}
$$

Let's start by reading and rearranging the data.

```{r}
# set header = TRUE read the first row in the csv as colnames.
# for every day, we will have observations corresponding to every half hour
d1 = read.csv('areac_data/accessi-orari-areac-2016-09-12-00_00_00.csv', header=T)
d2 = read.csv('areac_data/accessi-orari-areac-2016-09-13-00_00_00.csv', header=T)
d3 = read.csv('areac_data/accessi-orari-areac-2016-09-14-00_00_00.csv', header=T)
d4 = read.csv('areac_data/accessi-orari-areac-2016-09-15-00_00_00.csv', header=T)
d5 = read.csv('areac_data/accessi-orari-areac-2016-09-16-00_00_00.csv', header=T)
d6 = read.csv('areac_data/accessi-orari-areac-2016-09-17-00_00_00.csv', header=T)
d7 = read.csv('areac_data/accessi-orari-areac-2016-09-18-00_00_00.csv', header=T)

# we bind the rows corresponding to every day.
week = rbind(d1[,2], d2[,2], d3[,2], d4[,2], d5[,2],
             d6[,2], d7[,2])
# plot
matplot(seq(0,47)/2, # every half-hour of the day
        t(week), # we want to plat the columns
        type='l',
        col=c(1,1,1,1,1,2,2), # colour weekdays and weekend-days differentlyy
        lty=1)

```

As you remember, we can actually choose whatever test statistic we may like: if the permutation scheme used during the test is likelihood-invariant, we will get in any case an exact test.
There are nevertheless better choices than others (in the sense that we have a higher power, as we saw in the previous lab).
Let's try to use the squared euclidean distance between the two sample mean vectors (admittedly a quite standard choice.)

```{r}
t1 = week[1:5,]
t2 = week[6:7,]

t1.mean = colMeans(t1)
t2.mean = colMeans(t2)

matplot(seq(0,47)/2,
        t(rbind(t1.mean,t2.mean)), 
        type='l', col=c(1,2), lty=1)
```

Let's compute the test statistic

```{r}
n1 = dim(t1)[1]
n2 = dim(t2)[1]
n  = n1 + n2

T20 = as.numeric(t(t1.mean-t2.mean) %*% (t1.mean-t2.mean))  # matrix (vector) product
T20
```

To perform our test, we need to compare the test statistic to its (permutational) distribution under $H_0$.

```{r}
# Estimating the permutational distribution under H0

T2 = numeric(B)
set.seed(seed)
for(perm in 1:B){
  # Random permutation of indexes
  # When we apply permutations in a multivariate case, we keep the units together
  # i.e., we only permute the rows of the data matrix
  t_pooled = rbind(t1,t2)
  permutation = sample(n)
  t_perm = t_pooled[permutation,]
  t1_perm = t_perm[1:n1,]
  t2_perm = t_perm[(n1+1):n,]
  
  # Evaluation of the test statistic on permuted data
  t1.mean_perm = colMeans(t1_perm)
  t2.mean_perm = colMeans(t2_perm)
  T2[perm]  = t(t1.mean_perm-t2.mean_perm) %*% (t1.mean_perm-t2.mean_perm) 
}
```

Let's now see the shape of the permutational distribution, compared with the computed test statistic (the green vertical line...)

```{r}
hist(T2,xlim=range(c(T2,T20)))
abline(v=T20,col=3,lwd=4)

plot(ecdf(T2))
abline(v=T20,col=3,lwd=4)
```

The P-value will be amazingly low... but let's try to calculate it nevertheless.
Recall the p-value is given by:
$$
p = B^{-1}\sum_{b=1}^B\mathbb{I}_{\{T_b \geq T_0 \}}
$$

```{r}
p_val = sum(T2>=T20)/B
p_val
```

With this P-value, I can say, with a level of confidence higher than $95 \%$ that weekdays and weekends are significantly different

let's see another case now...


## Center of symmetry of one multivariate population 


We're still in Milan, but now we want to check if the humidity of ($4$) summer months is significantly different from the "non-comfort" threshold level of $(65\%)$.

In other words, being ($i$ represents the sample's $i$th datum )
$$
\mathbf{Y}_{i} \overset{iid}{\sim} \mathbf{Y} \in \mathbb{R}^{4}
$$
, I want to test

$$
H_0: \mathbb{E}[\mathbf{Y}] = \mathbf{\mu}_{0}\;vs\;H_1:\mathbb{E}[\mathbf{Y}] \neq \mathbf{\mu}_0
$$
Let's read and plot the data also here.
We will have 4 observations ($p=4$) for 7 different year ($n=7$).

```{r}
hum = read.csv2('humidity_data/307_Umidita_relativa_2008_2014.csv', header=T)
hum = hum$Media  # we use only the mean humidity
hum = matrix(hum,  # cast as matrix
             ncol=12, byrow=T)[,6:9] # so we have n = 7 and p =4

boxplot(hum)
matplot(t(hum),
        type='l',
        lty=1)

```

What is a reasonable permutation scheme to implement, and thus what is a reasonable test to perform? It is actually harder with respect to two-sample tests... but if we **assume the distribution to be symmetric**, reflections are permutationally invariant, and thus I can easily test this! Let's define the center of symmetry

```{r}
mu0      = c(65, 65, 65, 65)
```

Let's compute the test statistic (here the squared distance between the sample mean and the hypothesised centre, but of course other choices are possible...)


```{r}
x.mean   = colMeans(hum)
n = dim(hum)[1]
p = dim(hum)[2]

T20 = as.numeric(t(x.mean-mu0) %*% (x.mean-mu0) )
```


And now, let's compute the permutational distribution!

```{r}
T2 = numeric(B) 
set.seed(seed)

for(perm in 1:B){
  # In this case we use changes of signs in place of permutations
  
  # Permuted dataset (reflection-based)
  signs.perm = rbinom(n, 1, 0.5)*2 - 1
  hum_perm = mu0 + (hum - mu0) * matrix(signs.perm, nrow=n,ncol=p,byrow=FALSE)  # hadamard product
  x.mean_perm = colMeans(hum_perm)
  T2[perm]  = t(x.mean_perm-mu0)  %*% (x.mean_perm-mu0) #n.b. also works without the t, but better to play it safe!
}
```

let's plot the permutational distribution of the test statistic

```{r}
hist(T2,xlim=range(c(T2,T20)))
abline(v=T20,col=3,lwd=4)

plot(ecdf(T2))
abline(v=T20,col=3,lwd=4)
```

and the p-value

```{r}
p_val <- sum(T2>=T20)/B
p_val

```

Also here, I can argue that the humidity during the summer months is, with a 95% level of confidence ( _i.e._ 5% of the times I will reject $H_0$ when it is actually true), significantly different from $65%$

Let's now see our last case...

## Two sample paired multivariate permutation test

In this case, we want to compare the temperature, humidity and wind-speed in 50 different days in Milan and Barcelona.
We deem $X_M$ the r.v. for the quantity of interest for the population of Milan with unknown distribution $F_M$ and $X_B$ with $F_B$ those of Barcelona.
We have data:
$$
\mathbf{x}_i \stackrel{iid}{\sim} F_M; i \in \{1, ..., N\}
$$
_et_
$$
\mathbf{y}_i \stackrel{iid}{\sim} F_B; i \in \{1, ..., N\}
$$
And, very importantly, the observations are paired:
$$
(\mathbf{x}_i, \mathbf{y}_i) \stackrel{iid}{\sim} F_M - F_B
$$

We test:
$$
H_0 : \mathbb{E}[X_M-X_B] = \mathbf{0} \, vs. H1: \mathbb{E}[X_M-X_B] \neq \mathbf{0}
$$

Let's read the data
```{r}
t1 <- read.table('meteo_data/barcellona.txt', header=T)
t2 <- read.table('meteo_data/milano.txt', header=T)

```

Let's try to explore the data... we can work with the paired differences and plot them.

```{r}
library(rgl)
open3d()
plot3d(t1-t2, size=3, col='orange', aspect = F)
points3d(0,0,0, size=6)

p  <- dim(t1)[2]
n1 <- dim(t1)[1]
n2 <- dim(t2)[1]
n <- n1+n2

```

In terms of permutation schemes (and testing strategies...) to follow, the best choice is to compute the differences between the two groups, assume their distribution to be symmetric, and then perform a centre of symmetry test.

What is the best test statistics for the test? let's see...

```{r}
t1.mean <- colMeans(t1)
t2.mean <- colMeans(t2)
t1.cov  <-  cov(t1)
t2.cov  <-  cov(t2)
Sp      <- ((n1-1)*t1.cov + (n2-1)*t2.cov)/(n1+n2-2)  # pooled cov matrix
Spinv   <- solve(Sp)

delta.0 <- c(0,0,0)

diff <- t1-t2
diff.mean <- colMeans(diff)
diff.cov <- cov(diff)
diff.invcov <- solve(diff.cov)
```

Let's start with the squared euclidean distance between the difference in means and the hypothesised value

```{r}
T20 <- as.numeric(t(diff.mean-delta.0)  %*% (diff.mean-delta.0))
```

And then, let's perform the test

```{r}
T2 <- numeric(B)
set.seed(seed)
for(perm in 1:B)
  {
  # Random permutation
  # obs: exchanging data within couples means changing the sign of the difference
  signs.perm <- rbinom(n1, 1, 0.5)*2 - 1
  
  diff_perm <- diff * matrix(signs.perm,nrow=n1,ncol=p,byrow=FALSE)
  diff.mean_perm <- colMeans(diff_perm)
  diff.cov_perm <- cov(diff_perm)
  diff.invcov_perm <- solve(diff.cov_perm)
  
  T2[perm] <- as.numeric(t(diff.mean_perm-delta.0) %*% (diff.mean_perm-delta.0))
  }
```
Distribution and pvalue

```{r}
# plotting the permutational distribution under H0
hist(T2,xlim=range(c(T2,T20)),breaks=100)
abline(v=T20,col=3,lwd=4)

plot(ecdf(T2))
abline(v=T20,col=3,lwd=4)


# p-value
p_val <- sum(T2>=T20)/B
p_val

```



Now, let's use the Mahalanobis distance, but "forgetting" about the covariance between the values

```{r}
T20 <- as.numeric( t(diff.mean-delta.0) %*% solve(diag(diag(diff.cov))) %*% (diff.mean-delta.0))
# Estimating the permutational distribution under H0
T2 <- numeric(B)
set.seed(seed)
for(perm in 1:B)
  {
  # Random permutation
  # obs: exchanging data within couples means changing the sign of the difference
  signs.perm <- rbinom(n1, 1, 0.5)*2 - 1
  
  diff_perm <- diff * matrix(signs.perm,nrow=n1,ncol=p,byrow=FALSE)
  diff.mean_perm <- colMeans(diff_perm)
  diff.cov_perm <- cov(diff_perm)
  diff.invcov_perm <- solve(diff.cov_perm)
  

  T2[perm] <- as.numeric((diff.mean_perm-delta.0) %*% solve(diag(diag(diff.cov_perm))) %*% (diff.mean_perm-delta.0))
  
}

# plotting the permutational distribution under H0
hist(T2,xlim=range(c(T2,T20)),breaks=100)
abline(v=T20,col=3,lwd=4)

plot(ecdf(T2))
abline(v=T20,col=3,lwd=4)


# p-value
p_val <- sum(T2>=T20)/B
p_val

```

and lastly, let's use the proper Mahalanobis distance

```{r}
T20 <- as.numeric((diff.mean-delta.0) %*% diff.invcov %*% (diff.mean-delta.0))



# Estimating the permutational distribution under H0

set.seed(seed)
T2 <- numeric(B)

for(perm in 1:B)
  {
  # Random permutation
  # obs: exchanging data within couples means changing the sign of the difference
  signs.perm <- rbinom(n1, 1, 0.5)*2 - 1
  
  diff_perm <- diff * matrix(signs.perm,nrow=n1,ncol=p,byrow=FALSE)
  diff.mean_perm <- colMeans(diff_perm)
  diff.cov_perm <- cov(diff_perm)
  diff.invcov_perm <- solve(diff.cov_perm)
  
  #T2[perm] <- as.numeric(n1 * (diff.mean_perm-delta.0) %*% (diff.mean_perm-delta.0))
  #T2[perm] <- as.numeric(n1 * (diff.mean_perm-delta.0) %*% solve(diag(diag(diff.cov_perm))) %*% (diff.mean_perm-delta.0))
  T2[perm] <- as.numeric(n1 * (diff.mean_perm-delta.0) %*% diff.invcov_perm %*% (diff.mean_perm-delta.0))
  }

# plotting the permutational distribution under H0
hist(T2,xlim=range(c(T2,T20)),breaks=100)
abline(v=T20,col=3,lwd=4)

plot(ecdf(T2))
abline(v=T20,col=3,lwd=4)


# p-value
p_val <- sum(T2>=T20)/B
p_val

```


##  Two sample paired multivariate permutation test - an alternative

If you read the _dispensa_ created by Professor Alessia Pini^[Unpublished, _cfr._ the material on Webeep.], you would find an alternative for this test. 

Instead of performing inference on the distribution of 
$$
X_M - X_B
$$
we treat them "separately", finding likelihood-invariant transformations under $H_0$
that respect the paired nature of the test.
We test:
$$
H_0: X_M \stackrel{d}{=} X_B \; vs. \;H_1: X_M \stackrel{d}{\neq} X_B
$$
And we have $2^N$ different permutations, since the **exchangeability is only within the pairs**.
We choose as test statistic the norm of the differences of the means of both samples.


```{r}
T20 <- norm(as.matrix(colMeans(t1) - apply(t2, MARGIN=2, FUN=mean)))
```


```{r}
T2 <- numeric(B)
p <- dim(t1)[2] # here naturally I can use t1 or t2.
n <- dim(t2)[1]
t.full <- rbind(t1, t2)
set.seed(seed)
for(perm in 1:B)
  {
  # Random permutation
  # N.B. exchangeability is only within pairs
  perm.indices.t1 <- seq(1, n) + n * rbinom(n,1, 0.5)
  t1.perm <- t.full[perm.indices.t1, ]
  t2.perm <- t.full[-perm.indices.t1,]
  
  T2[perm] <- norm(as.matrix(((colMeans(t1.perm)) - colMeans(t2.perm))))
}

hist(T2,xlim=range(c(T2,T20)),breaks=100)
abline(v=T20,col=3,lwd=4)

plot(ecdf(T2))
abline(v=T20,col=3,lwd=4)


# p-value
p_val <- sum(T2>=T20)/B
p_val
```

