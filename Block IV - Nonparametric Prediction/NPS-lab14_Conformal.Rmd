---
title: "Scalar, Multivariate and Functional Conformal Prediction "
date: 2022/12/1
author: "Nonparametric statistics ay 2022/2023"
output:
  
  html_document: 
    df_print: paged
  pdf_document: default
  html_notebook: 
    df_print: paged
  word_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(webgl = hook_webgl)
ggplot2::theme_set(ggplot2::theme_bw())
```

```{css, echo=FALSE}
.extracode {
background-color: lightblue;
}
```


## Some More Topics on Conformal Prediction

Let's read the packages we need
```{r}
library(dbscan)
```


You have seen with Prof. Vantini the basics of Conformal Prediction: What is it, why is it so cool, and how it works.
The objective of this part is to show you some more advanced topics...

For instance, what happens if I decide to use CP with a bimodal distribution?
(BEWARE: Contrary to what Prof. Vantini showed you, I am using the test point too to compute NCMs. Your prediction intervals will be still valid... you can actually prove why this is the case.)


```{r}
set.seed(1991)
n_b=n=1000
grid_factor=1.25
n_grid=200
alpha=.1

y_b=c(rnorm(n_b/2,mean = -2),rnorm(n_b/2,mean=2))
hist(y_b)

```

Let's use the classical NCM (absolute value of the difference from the mean...)

```{r}
wrapper_full=function(grid_point){
  
  aug_y=c(grid_point,y_b)
  mu=mean(aug_y)
  ncm=abs(mu - aug_y) 
  sum((ncm[-1]>=ncm[1]))/(n+1)
  
}

test_grid=seq(-grid_factor*max(abs(y_b)),+grid_factor*max(abs(y_b)),length.out = n_grid)
pval_fun=sapply(test_grid,wrapper_full)
plot(test_grid,pval_fun,type='l')
abline(v=c(-2,2),col='red')
```


In this case the P-value function is unimodal, so I can just compute the range

```{r}
index_in=pval_fun>alpha


PI=range(test_grid[index_in])
plot(test_grid,pval_fun,type='l')
abline(v=c(-2,2),col='red')
abline(v=PI,col='blue')
hist(y_b)
abline(v=PI,col='blue')
```

Not really great, isn't it? Can I do better? What about a more local NCM? average euclidean distance from k-nn

```{r}
pval_fun=numeric(n_grid)
k_s=0.25

wrapper_knn=function(grid_point){
  
  aug_y=c(grid_point,y_b)
  ncm=kNNdist(matrix(aug_y),k_s*n)
  sum((ncm[-1]>=ncm[1]))/(n_b+1)
  
} 
```


This is going to take a bit, so let's something we know very well

```{r}
library(pbapply)
pval_fun=pbsapply(test_grid,wrapper_knn)


plot(test_grid,pval_fun,type='l')
abline(v=c(-2,2),col='red')

```

Let's now identify the boundaries of the intervals...

```{r}
#let's see when I switch from true to false:
index_in=pval_fun>0.2
breaks=test_grid[as.logical(c(0,abs(diff(index_in))))]
plot(test_grid,pval_fun,type='l')
abline(v=c(-2,2),col='red')
abline(v=breaks,col='blue')
hist(y_b)
abline(v=breaks,col='blue')

```

## Multivariate Conformal Prediction

You have seen in class that the theory behind Conformal Prediction works for objects belonging to metric spaces... which opens quite a LOT of possibilities!

Let's generate from a bivariate T

```{r}
set.seed(19921126)
n=40
y_biv=cbind(rt(n,2),rt(n,2))

n_grid=200
plot(y_biv[,1],y_biv[,2])

```

The Full Conformal framework is quite straightforward... instead of computing the pvalue function on a line, I am doing it on a plane...

```{r}
n_grid=200

test_grid_x=seq(-grid_factor*max(abs(y_biv[,1])),+grid_factor*max(abs(y_biv[,1])),length.out = n_grid)
test_grid_y=seq(-grid_factor*max(abs(y_biv[,2])),+grid_factor*max(abs(y_biv[,2])),length.out = n_grid)
xy_surface=expand.grid(test_grid_x,test_grid_y)
wrapper_multi_conf=function(test_point){
  
  newdata=rbind(test_point,y_biv)
  depth_surface_vec=rowSums(t(t(newdata)-colMeans(newdata))^2) #In this case I am using the L^2 norm...
  sum(depth_surface_vec[-1]>=depth_surface_vec[1])/(n+1)
}


pval_surf=pbapply(xy_surface,1,wrapper_multi_conf)
```

Let's plot it...

```{r}
data_plot=cbind(pval_surf,xy_surface)

library(ggplot2)
ggplot() + 
scale_color_continuous()+
geom_tile(data=data_plot, aes(Var1, Var2, fill= pval_surf)) +
geom_point(data=data.frame(y_biv), aes(X1,X2)) + 
  ylim(-5,5)+
  xlim(-5,5)

```


And let's plot the prediction set

```{r}
p_set=xy_surface[pval_surf>alpha,]
poly_points=p_set[chull(p_set),]

ggplot() + 
  geom_tile(data=data_plot, aes(Var1, Var2, fill= pval_surf)) +
  geom_point(data=data.frame(y_biv), aes(X1,X2)) + 
  geom_polygon(data=poly_points,aes(Var1,Var2),color='red',size=1,alpha=0.01)+
  ylim(-5,5)+
  xlim(-5,5)

```


We can of course use a different NCM, such as a Mahalanobis depth!

```{r}
library(roahd)
wrapper_multi_conf_mah=function(test_point){
  
  newdata=rbind(test_point,y_biv)
  depth_surface_vec= mahalanobis(newdata,colMeans(newdata),cov = cov(newdata))
  sum(depth_surface_vec[-1]>=depth_surface_vec[1])/(n+1)
}

pval_surf=pbapply(xy_surface,1,wrapper_multi_conf_mah)

```

Let's plot this P-value surface
```{r}
#let's plot this p-value surface:
data_plot=cbind(pval_surf,xy_surface)


ggplot() + 
geom_tile(data=data_plot, aes(Var1, Var2, fill= pval_surf)) +
  geom_point(data=data.frame(y_biv), aes(X1,X2))+
xlim(-5,5)+
ylim(-5,5)
```

And, we can of course plot the prediction set...

```{r}
#let's define the prediction set...
p_set=xy_surface[pval_surf>alpha,]
poly_points=p_set[chull(p_set),]

ggplot() + 
  geom_tile(data=data_plot, aes(Var1, Var2, fill= pval_surf)) +
  geom_point(data=data.frame(y_biv), aes(X1,X2)) + 
  geom_polygon(data=poly_points,aes(Var1,Var2),color='red',size=1,alpha=0.01)+
xlim(-5,5)+
ylim(-5,5)
```


Can I do split Conformal in this case? of course!

```{r}
i1=sample(1:n,n/2)
t_set=y_biv[i1,]
c_set=y_biv[-i1,]
mu=colMeans(t_set)
ncm=sqrt(rowSums(c_set-mu)^2)
ncm_sort=c(sort(ncm),Inf)
d=ncm_sort[ceiling((n/2 + 1)*(1-alpha))]

```

What is the shape of the region where $d_euc(\mu,y)>d$

```{r}
plot(y_biv,xlim = c(-10,10),ylim = c(-10,10))


angle_grid=seq(0,(2*pi),length.out = 1000)
circle_points=data.frame(x=d*sin(angle_grid) + mu[1],y=d*cos(angle_grid) + mu[2])
polygon(circle_points)

```

But of course, as it is the case with every "Split" conformal...

```{r}
plot(y_biv,xlim = c(-10,10),ylim = c(-10,10))
polygon(circle_points)

i1=sample(1:n,n/2)
t_set=y_biv[i1,]
c_set=y_biv[-i1,]
mu=colMeans(t_set)
ncm=sqrt(rowSums(c_set-mu)^2)
ncm_sort=c(sort(ncm),Inf)
d=ncm_sort[ceiling((n/2 + 1)*(1-alpha))]
circle_points=data.frame(x=d*sin(angle_grid) + mu[1],y=d*cos(angle_grid) + mu[2])
polygon(circle_points,border='blue')

i1=sample(1:n,n/2)
t_set=y_biv[i1,]
c_set=y_biv[-i1,]
mu=colMeans(t_set)
ncm=sqrt(rowSums(c_set-mu)^2)
ncm_sort=c(sort(ncm),Inf)
d=ncm_sort[ceiling((n/2 + 1)*(1-alpha))]
circle_points=data.frame(x=d*sin(angle_grid) + mu[1],y=d*cos(angle_grid) + mu[2])
polygon(circle_points,border='red')

i1=sample(1:n,n/2)
t_set=y_biv[i1,]
c_set=y_biv[-i1,]
mu=colMeans(t_set)
ncm=sqrt(rowSums(c_set-mu)^2)
ncm_sort=c(sort(ncm),Inf)
d=ncm_sort[ceiling((n/2 + 1)*(1-alpha))]
circle_points=data.frame(x=d*sin(angle_grid) + mu[1],y=d*cos(angle_grid) + mu[2])
polygon(circle_points,border='green')
```


Ok, we've seen plenty of funny examples, but how do I generate predictions from my very complex regression models?
Luckly for you, there's a package for it!

```{r}
# Make sure to install devtools as well and if you're not authenticated you
# should create a token by running
# usethis::create_github_token()
# and then run
# usethis::edit_r_environ()
# and type
# GITHUB_PAT=THE_TOKEN_YOU_GENERATED
# Restart R for changes to take effect

# devtools::install_github(repo="ryantibs/conformal", subdir="conformalInference")
library(conformalInference)

```


```{r}
load(here::here('Block IV - Nonparametric Prediction','nlr_data.rda'))
attach(Prestige)
```

I know how to generate predictions from a linear model...

```{r}
model_poly=lm(prestige ~ income)
income.grid=seq(range(income)[1],range(income)[2],by=100)

preds=predict(model_poly,list(income=income.grid),se=T)

plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Linear')
lines(income.grid,preds$fit ,lwd =2, col =" blue")

```

How to do it a Conformal setting? The ConformalInference package has a "functional programming" soul...
So, I can "extract" the training and the prediction function...

```{r}
lm_train=lm.funs(intercept = T)$train.fun
lm_predict=lm.funs(intercept = T)$predict.fun

```

And then feed them to a "conformalisator"!

```{r}
c_preds=conformal.pred(income,prestige,income.grid,alpha=0.05,verbose=T,train.fun = lm_train,predict.fun = lm_predict,num.grid.pts = 200)

plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Linear')
lines(income.grid,c_preds$pred ,lwd =2, col ="red",lty=3)
matlines(income.grid ,cbind(c_preds$up,c_preds$lo) ,lwd =1, col =" blue",lty =3)

```

What should I do to include additional regressors (or, for example, a set of polynomials?)

```{r}
model_poly=lm(prestige ~ poly(income,degree=2))
income.grid=seq(range(income)[1],range(income)[2],by=100)

preds=predict(model_poly,list(income=income.grid),se=T)

plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Polynomial')
lines(income.grid,preds$fit ,lwd =2, col =" blue")
```

The only thing I need to remember is that I need to feed the design matrix to the "conformalisator"

```{r}
design_matrix=matrix(poly(income,degree=2),ncol=2)
pred_grid=matrix(poly(income.grid,degree=2,coefs = attr(poly(income,degree=2),"coefs") ),ncol=2)

c_preds=conformal.pred(design_matrix,prestige,pred_grid,alpha=0.05,verbose=T,train.fun = lm_train,predict.fun = lm_predict,num.grid.pts = 200)

plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Polynomial')

lines(income.grid,c_preds$pred ,lwd =2, col ="red",lty=3)
matlines(income.grid ,cbind(c_preds$up,c_preds$lo) ,lwd =1, col =" blue",lty =3)
```

I can of course run everything in a split conformal framework... with the same caveats as before...

```{r}
plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Polynomial')
lines(income.grid,preds$fit ,lwd =2, col =" blue")

c_preds_split=conformal.pred.split(design_matrix,prestige,pred_grid,alpha=0.05,verbose=T,train.fun = lm_train,predict.fun = lm_predict,)

lines(income.grid,c_preds_split$pred ,lwd =2, col ="red",lty=3)
matlines(income.grid ,cbind(c_preds_split$up,c_preds_split$lo) ,lwd =1, col =" red",lty =3)


c_preds_split=conformal.pred.split(design_matrix,prestige,pred_grid,alpha=0.05,verbose=T,train.fun = lm_train,predict.fun = lm_predict)

lines(income.grid,c_preds_split$pred ,lwd =2, col ="green",lty=3)
matlines(income.grid ,cbind(c_preds_split$up,c_preds_split$lo) ,lwd =1, col =" green",lty =3)

c_preds_split=conformal.pred.split(design_matrix,prestige,pred_grid,alpha=0.05,verbose=T,train.fun = lm_train,predict.fun = lm_predict)

lines(income.grid,c_preds_split$pred ,lwd =2, col ="orange",lty=3)
matlines(income.grid ,cbind(c_preds_split$up,c_preds_split$lo) ,lwd =1, col =" orange",lty =3)
```

If I remain in a linear framework, when I can reconduce my model to something that can be estimated using lm, everything is super-easy

```{r}
library(splines)
br=c(quantile(income,probs = c(0.2,0.4,0.6,0.8)),15000)
model_cut=lm(prestige ~ bs(income, degree=3,knots=br))

preds=predict(model_cut,list(income=income.grid),se=T)

plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Spline' )
lines(income.grid,preds$fit ,lwd =2, col =" blue")
```

Again, I just need to remember that I must use the design matrix

```{r}
design_matrix=bs(income, degree=3,knots=br)
pred_grid=matrix(bs(income.grid,degree=3,knots=br),nrow=length(income.grid))

c_preds=conformal.pred(design_matrix,prestige,pred_grid,alpha=0.05,verbose=T,train.fun = lm_train,predict.fun = lm_predict,num.grid.pts = 200)


plot(income ,prestige ,xlim=range(income.grid) ,cex =.5, col =" darkgrey ",main='Spline' )
lines(income.grid,c_preds$pred ,lwd =2, col ="red",lty=3)
matlines(income.grid ,cbind(c_preds$up,c_preds$lo) ,lwd =1, col =" blue",lty =3)

```

In a "nonlm" case I need to create custom functions... namely something that yields as output the prediction model object, that will be then fed to the "predict" function, that needs to output the value predicted

Let's see what happens with smoothing splines, for instance.

```{r}
fit=smooth.spline(income,prestige,cv=T)
plot(income ,prestige,cex =.5, col =" darkgrey ")
lines(fit,col="blue",lwd=2)
opt = fit$df
opt
fit$lambda

```

Now define the functions

```{r}
train_ss=function(x,y,out=NULL){
  smooth.spline(x,y,df=opt)
}

predict_ss=function(obj, new_x){
  predict(obj,new_x)$y
}

pippo=train_ss(income,prestige)

predict_ss(pippo,income)

```

And let's see them at work

```{r}
c_preds=conformal.pred(income,prestige,income.grid,alpha=0.05,verbose=T,train.fun = train_ss ,predict.fun = predict_ss,num.grid.pts = 200)

plot(income ,prestige,cex =.5, col =" darkgrey ", main='smoothing spline')
lines(income.grid,c_preds$pred ,lwd =2, col ="red",lty=3)
matlines(income.grid ,cbind(c_preds$up,c_preds$lo) ,lwd =1, col =" blue",lty =3)


```

Let's try a more complex case... GAMs! I will provide to you an admittedly very quick, but very dirty solution...

```{r}
library(mgcv)
model_gam=gam(prestige ~ s(education,bs='cr') + s(income,bs='cr'))

education.grid=seq(range(education)[1],range(education)[2],length.out = 100)
income.grid=seq(range(income)[1],range(income)[2],length.out = 100)


grid=expand.grid(education.grid,income.grid)
names(grid)=c('education','income')
pred=predict(model_gam,newdata=grid)
```

```{r, webgl=T}
library(rgl)

persp3d(education.grid,income.grid,pred,col='yellow')
points3d(education,income,prestige,col='black',size=5)
```

Now, lets define the functions...

```{r}
train_gam=function(x,y,out=NULL){
  colnames(x)=c('var1','var2')
  train_data=data.frame(y,x)
  model_gam=gam(y ~ s(var1,bs='cr') + s(var2,bs='cr'),data=train_data)
}


predict_gam=function(obj, new_x){
  new_x=data.frame(new_x)
  colnames(new_x)=c('var1','var2')
  predict.gam(obj,new_x)
}
```

And, let's generate our predictions

```{r}
c_preds=conformal.pred(cbind(education,income),prestige,c(median(education),median(income)),alpha=0.05,verbose=T,train.fun = train_gam ,predict.fun = predict_gam,num.grid.pts = 200)
c_preds

c_preds=conformal.pred.split(cbind(education,income),prestige,c(median(education),median(income)),alpha=0.05,verbose=T,train.fun = train_gam ,predict.fun = predict_gam)
c_preds

```

what to do in a multivariate case (i.e. a multivariate response)? Luckly for you there's a package for that!

```{r}
#install.packages('conformalInference.multi')
library(conformalInference.multi)
```

Let's generate some multivariate data

```{r}
n=25
p=4
q=2


mu=rep(0,p)
x = mvtnorm::rmvnorm(n, mu)
beta<-sapply(1:q, function(k) c(mvtnorm::rmvnorm(1,mu)))
y = x%*%beta + t(mvtnorm::rmvnorm(q,1:(n)))
x0=x[n,]
y0=y[n,]

n0<-nrow(y0)
q<-ncol(y)
```

Let's declare the fitting and prediction function

```{r}
fun=mean_multi()

```

And then run the usual full conformal algorithm

```{r}
final.full=conformal.multidim.full(x, y, x0, fun$train.fun,
                                fun$predict.fun, score="l2",
                                num.grid.pts.dim=100, grid.factor=1.25,
                                verbose=FALSE)

plot_multidim(final.full)
```


I can customise the ncm, of course

```{r}
final.full=conformal.multidim.full(x, y, x0, fun$train.fun,
                                fun$predict.fun, score="max",
                                num.grid.pts.dim=100, grid.factor=1.25,
                                verbose=FALSE)

plot_multidim(final.full)
```


And what about functional data?

```{r}
library(fda)
data=growth #data from the berkeley growth study...
```

And let's plot my curves...

```{r}
matplot(data$age,data$hgtm, type='l',col='blue')
matlines(data$age,data$hgtf, type='l',col='red')
```


```{r}
alpha=.1
ber_m = t(data$hgtm)
n=nrow(ber_m)

i1=sample(1:n,n/2)
t_set=ber_m[i1,]
c_set=ber_m[-i1,]
mu=colMeans(t_set)
res=c_set-mu
ncm=apply(res,2,max)
ncm_sort=c(sort(ncm),Inf)
d=ncm_sort[ceiling((n/2 + 1)*(1-alpha))]
matplot(cbind(mu,mu+d,mu-d),type='l')
```

What if I want to scale the amplitude of this?

```{r}
S=apply(t_set,2,var)
res=(c_set-mu)/S
ncm=apply(res,2,max)
ncm_sort=c(sort(ncm),Inf)
d=ncm_sort[ceiling((n/2 + 1)*(1-alpha))]
matplot(cbind(mu,mu+(d*S),mu-(d*S)),type='l')
```

This can also be implemented in a faster way, by using the `ConformalInference.fd` package


```{r}
#install.packages("conformalInference.fd")
library(conformalInference.fd)

```

conformalinference.fd works best with the roahd package, then

```{r}
library(roahd)
ber_m_fd=fData(data$age,ber_m)
plot(ber_m_fd)

```


```{r}
x0=list(as.list(grid))
fun=mean_lists()
final.mfData = conformal.fun.split(NULL,NULL, ber_m_fd,NULL, x0, fun$train.fun, fun$predict.fun,
                             alpha=0.1,
                             split=NULL, seed=FALSE, randomized=FALSE,seed.rand=FALSE,
                             verbose=TRUE, rho=0.5,s.type="identity")

plot_fun(final.mfData)
```

And, of course, I can choose whatever modulation I want

```{r}
final.mfData = conformal.fun.split(NULL,NULL, ber_m_fd,NULL, x0, fun$train.fun, fun$predict.fun,
                             alpha=0.1,
                             split=NULL, seed=FALSE, randomized=FALSE,seed.rand=FALSE,
                             verbose=TRUE, rho=0.5,s.type="st-dev")

plot_fun(final.mfData)
```

```{r}
final.mfData = conformal.fun.split(NULL,NULL, ber_m_fd,NULL, x0, fun$train.fun, fun$predict.fun,
                             alpha=0.1,
                             split=NULL, seed=FALSE, randomized=FALSE,seed.rand=FALSE,
                             verbose=TRUE, rho=0.5,s.type="alpha-max")

plot_fun(final.mfData)
```

